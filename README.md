# FMCG ETL Pipeline â€“ Retail Merger Use Case
## ğŸ“Œ Project Overview
In the FMCG domain, when a large retail company acquires a smaller one, consolidating their data becomes critical for unified reporting and analytics.
This project builds an ETL pipeline to integrate and transform data from both companies into a single Lakehouse architecture following the Medallion framework.
The pipeline ensures:
- Seamless ingestion of structured and semi-structured data
- Standardized transformations for consistency
- Scalable storage in Amazon S3
- Unified reporting via BI dashboards

## ğŸ—ï¸ Architecture
### Medallion Layers
- **Bronze Layer:** Raw data ingestion from both companies (CSV, JSON, SQL dumps).
- **Silver Layer:** Cleaned and standardized data (schema alignment, deduplication, enrichment).
- **Gold Layer:** Business-ready aggregated datasets for analytics and dashboards.

### Tech Stack
| Component | Technology Used | 
| --- | --- |
| Programming | Python, SQL | 
| Storage | Amazon S3 + Delta Lake | 
| Processing | Databricks (Apache Spark) | 
| Architecture | Medallion (Bronze, Silver, Gold) | 
| Visualization | BI Dashboard (Power BI / Tableau) | 
| Orchestration | Databricks Jobs + Genie | 

## âš™ï¸ Workflow
1. **Data Ingestion**
    - Source data from legacy systems (CSV, JSON, SQL dumps).
    - Load into Databricks Bronze tables (Delta Lake on S3).
2. **Data Transformation**
    - Use PySpark notebooks for schema harmonization, deduplication, and enrichment.
    - Apply SQL transformations for business rules.
    - Store results in Silver tables.
3. **Data Aggregation**
    - Build curated datasets (sales, inventory, customer insights).
    - Store in Gold tables for analytics.
4. **Visualization**
    - Connect BI tools (Power BI, Tableau) to Gold layer.
    - Deliver dashboards for executives and analysts.
5. **Orchestration**
    - Schedule and monitor pipelines using Databricks Jobs.
    - Integrate Genie for workflow automation.

## ğŸ“Š Example Use Case
- Consolidating sales transactions from both companies.
- Harmonizing product catalogs (different SKUs, naming conventions).
- Building a unified customer loyalty dashboard.

## ğŸš€ Getting Started
### Prerequisites
- Databricks Workspace
- AWS S3 bucket configured
- Python 3.9+ environment
- BI tool (Power BI / Tableau)

### Setup
- **Clone repository**
    - git clone https://github.com/your-org/fmcg-etl-pipeline.git
    - cd fmcg-etl-pipeline

- **Install dependencies**
    - pip install -r requirements.txt


### Run Pipeline
    - Trigger ETL job
    - python etl_pipeline.py



## ğŸ“‚ Repository Structure
fmcg-etl-pipeline/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ etl_pipeline.py
â”‚â”€â”€ configs/
â”‚   â””â”€â”€ aws_config.json
â”‚â”€â”€ notebooks/
â”‚   â””â”€â”€ data_cleaning.ipynb
â”‚â”€â”€ dashboards/
â”‚   â””â”€â”€ sales_dashboard.pbix
â”‚â”€â”€ docs/
â”‚   â””â”€â”€ architecture_diagram.png



## âœ… Deliverables
- Consolidated Lakehouse with Bronze, Silver, Gold layers
- Automated ETL pipeline on Databricks
- BI dashboards for FMCG insights
- Documentation for reproducibility


